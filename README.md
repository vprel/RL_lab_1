#### Reinforcement learning coursework

The notebook is split into three parts. 

The first part makes us examine reward graphs of different RL algorithms, answering why agent performance differs and any other observations we deem to be interesting. 

The second part is implementing a Temporal-Difference algorithm, in my case SARSA(λ), and then beat the Q-Learning based agents rewards. Furthermore, I added extra aspects to the algorithm such as Dutch Eligibility trace updates, α decay and ε decay.

The final part is similar to the first where we answer questions regarding our implementation.
